{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 多头注意力\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力层，用于计算多个注意力头的输出\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # 除取整\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # （输入的特征维度，输出的特征维度）\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        计算多头注意力的输出\n",
    "        \"\"\"\n",
    "        # Q (N,n_head,S,d_k)\n",
    "        # K (N,n_head,S,d_k)\n",
    "        # V (N,n_head,S,d_k)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # attn_scores (N,n_head,S,S)\n",
    "        if mask is not None: \n",
    "            # mask (N,1,1,S)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9) # attn_scores里mask为0的地方，用负无穷填充\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        # attn_probs (N,n_head,S,S)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        # output (N,n_head,S,d_k)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        分割多头注意力的输入，将输入的特征维度分割成多个头\n",
    "        \"\"\"\n",
    "        # (N,S,D)\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        # (N,S,n_head,d_k)\n",
    "        # (N,n_head,S,d_k)\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        组合多头注意力的输出，将多个头的输出拼接起来\n",
    "        \"\"\"\n",
    "        # x (N,n_head,S,d_k)\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q (N,S,D)\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        # Q (N,n_head,S,d_k)\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        # attn_output (N,n_head,S,d_k)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "d_model=16\n",
    "n_heads=4\n",
    "attention=MultiHeadAttention(d_model=d_model,num_heads=n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0737,  0.1454, -0.2397,  0.1644, -0.2396, -0.0370, -0.2983,  0.0133,\n",
      "         -0.1099, -0.2542, -0.2101, -0.2282, -0.0183, -0.0015,  0.0715,  0.0826],\n",
      "        [-0.1173,  0.2360, -0.1877,  0.2655, -0.2713, -0.1749, -0.2866, -0.0064,\n",
      "         -0.0932, -0.1685, -0.2030, -0.2352, -0.1353,  0.0887,  0.1380,  0.2580],\n",
      "        [-0.0735,  0.1678, -0.2673,  0.2655, -0.1810, -0.0833, -0.2807, -0.0929,\n",
      "         -0.2139, -0.1899, -0.1074, -0.2367, -0.1392,  0.0705,  0.1032,  0.2191],\n",
      "        [-0.0651,  0.1655, -0.2487,  0.2442, -0.1902, -0.0869, -0.2930, -0.0966,\n",
      "         -0.1672, -0.1772, -0.1552, -0.2677, -0.1435,  0.0510,  0.0936,  0.2404]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.0735,  0.1678, -0.2673,  0.2655, -0.1810, -0.0833, -0.2807, -0.0929,\n",
      "         -0.2139, -0.1899, -0.1074, -0.2367, -0.1392,  0.0705,  0.1032,  0.2191],\n",
      "        [-0.0737,  0.1454, -0.2397,  0.1644, -0.2396, -0.0370, -0.2983,  0.0133,\n",
      "         -0.1099, -0.2542, -0.2101, -0.2282, -0.0183, -0.0015,  0.0715,  0.0826],\n",
      "        [-0.1173,  0.2360, -0.1877,  0.2655, -0.2713, -0.1749, -0.2866, -0.0064,\n",
      "         -0.0932, -0.1685, -0.2030, -0.2352, -0.1353,  0.0887,  0.1380,  0.2580],\n",
      "        [-0.0651,  0.1655, -0.2487,  0.2442, -0.1902, -0.0869, -0.2930, -0.0966,\n",
      "         -0.1672, -0.1772, -0.1552, -0.2677, -0.1435,  0.0510,  0.0936,  0.2404]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size=1\n",
    "seq_len=4\n",
    "t1=torch.randn(1,d_model)\n",
    "t2=torch.randn(1,d_model)\n",
    "t3=torch.randn(1,d_model)\n",
    "t4=torch.randn(1,d_model)\n",
    "x1=torch.stack([t1,t2,t3,t4],dim=1)\n",
    "x2=torch.stack([t3,t1,t2,t4],dim=1)\n",
    "output=attention(x1,x1,x1)\n",
    "print(output[0])\n",
    "reversed_output=attention(x2,x2,x2)\n",
    "print(reversed_output[0])\n",
    "# 可以看出。更换attention的输入token顺序时（没有mask的情况下），输出的值也只是换了个顺序，但对应的值并没有变化。\n",
    "# 导致解码时，若在最后一个token相同时，即使交换前面token的顺序，decode出来的token是一样的（attention value的输出来说，只是换了一下attention_output1=q1k1V1+q1k2V2+...中的加法顺序）。\n",
    "# 所以需要在嵌入时，加上位置编码。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
