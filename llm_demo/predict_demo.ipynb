{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 设置hf-mirror镜像站，用于下载bert-base-chinese模型\n",
    "import os\n",
    "os.environ['HF_ENDPOINT']='https://hf-mirror.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM,AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "device='cuda:0'\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    151645,\n",
       "    151643\n",
       "  ],\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_k\": 20,\n",
       "  \"top_p\": 0.8\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! A large language model (LLM) is a type of artificial intelligence that can produce human-like text or perform tasks such as generating natural language responses. These models use large amounts of data and advanced algorithms to understand the context and generate coherent and meaningful responses. LLMs have been used in various applications, including virtual assistants, chatbots, and natural language processing for tasks like machine translation, summarization, and question answering. They also play an important role in AI research and development, particularly in areas like language understanding, natural language generation, and robotics.\n"
     ]
    }
   ],
   "source": [
    "# Qwen官方例子\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # 加上<|im_start|>assistant\\n\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "# 这里的内部实现应该是一个for循环，一个token一个token地生成\n",
    "with torch.inference_mode():\n",
    "    model_output_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, model_output_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过tokenizer的chat模板把messages更改为对话形式。\n",
    "# 让LLM在这个文本上做续写，就是chat版本模型干的事\n",
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
       "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
       "             13, 151645,    198, 151644,    872,    198,  35127,    752,    264,\n",
       "           2805,  16800,    311,   3460,   4128,   1614,     13, 151645,    198,\n",
       "         151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过tokenizer把刚刚的text分词并转化为数字id\n",
    "# 得到input_ids和attention_mask\n",
    "# 这里的attention_mask全是1\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  35127,    752,    264,\n",
      "           2805,  16800,    311,   3460,   4128,   1614,     13, 151645,    198,\n",
      "         151644,  77091,    198,     32,   3460,   4128,   1614,    374,    264,\n",
      "            943,    315,  20443,  11229,    429,    646,   8193,   3738,  12681,\n",
      "           1467,   3118,    389,  11127,   3897,    311,    432,     13,   4220,\n",
      "           4119,    525,   6188,    311,  55359,    279,   5810,   4128,   8692,\n",
      "          16928,    315,  12677,    323,    646,   6923,  55787,    323,   2266,\n",
      "           1832,   8311,  14507,    304,   5257,  30476,   1741,    438,   1467,\n",
      "           9471,     11,  28285,   2022,     11,   3405,  35764,     11,    323,\n",
      "            803,     13,  20286,   4128,   4119,    614,   1012,  13570,   1483,\n",
      "            304,   5043,   1075,   5662,  14468,     11,  25975,   6358,     11,\n",
      "          27682,   5942,     11,    323,    803,     13, 151645]],\n",
      "       device='cuda:0')\n",
      "151645\n"
     ]
    }
   ],
   "source": [
    "# decoder模型输出的ids\n",
    "# 可以发现前面的ids是一样的\n",
    "print(model_output_ids)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Large language models (LLMs) are artificial intelligence systems that can generate human-like text based on a pre-defined set of rules or patterns. These models are designed to mimic the complexity and creativity of human language, allowing them to understand context, learn from previous interactions, and produce coherent responses. LLMs have become increasingly popular in fields such as natural language processing, machine translation, chatbots, and virtual assistants due to their ability to process vast amounts of data quickly and generate meaningful outputs. They have also been used for tasks like image captioning, sentiment analysis, and question answering, demonstrating their potential to automate complex tasks and improve efficiency across various industries.<|im_end|>\n",
      "\n",
      "eos_token:  <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 把模型的输出decode出来看看\n",
    "print(tokenizer.decode(model_output_ids[0]))\n",
    "print(f'\\neos_token:  {tokenizer.eos_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 39])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型forward出来的shape是(batch_size,seq_len,vocab_size)\n",
    "with torch.inference_mode():\n",
    "    model.forward(**model_inputs)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入的sentence：\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "输出的sentence：\n",
      "Certainly! A large language model, or LLM, is a type of artificial intelligence that can generate human-like text based on a given input. These models are designed to be highly accurate, creative, and adaptable, making them useful in a wide range of applications, including but not limited to:\n",
      "\n",
      "1. **Chatbots**: Large language models are often used in chatbots to provide customer service, answer questions, and engage in conversations with users.\n",
      "\n",
      "2. **Text Generation**: They can generate text that is coherent, creative, and even original, which is useful for tasks such as writing articles, creating stories, or even writing books.\n",
      "\n",
      "3. **Sentiment Analysis**: Large language models can analyze and interpret the sentiment of text, which is useful in areas like social media monitoring, customer feedback analysis, and market research.\n",
      "\n",
      "4. **NLP (Natural Language Processing)**: They can process and understand natural language, which is crucial for tasks like language translation, summarization, and text summarization.\n",
      "\n",
      "5. **AI Chatbots**: Large language models are often used in AI chatbots to provide more natural and conversational interactions with users.\n",
      "\n",
      "6. **Educational Tools**: They can be used in educational settings to generate text that is engaging and interactive, which can help students learn and retain information more effectively.\n",
      "\n",
      "7. **Artificial Intelligence**: Large language models are used in AI applications, such as image recognition, speech recognition, and natural language processing.\n",
      "\n",
      "8. **Medical Research**: They can analyze and interpret medical data, which is useful in areas like drug discovery, disease diagnosis, and personalized medicine.\n",
      "\n",
      "9. **Legal Research**: They can generate legal documents and other legal materials based on the input provided.\n",
      "\n",
      "10. **Customer Service**: Large language models can be used in customer service to provide personalized and helpful responses to customers.\n",
      "\n",
      "Large language models are designed to be highly accurate, creative, and adaptable, making them a powerful tool for various applications. They are also subject to ethical considerations, such as the potential for bias and the impact on employment and privacy.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 一个简单的预测函数，一个token一个token地生成，直到达到最大长度或eos_token。\n",
    "# 这里没有用到temperature、topk、top-p等解码参数\n",
    "@torch.inference_mode()\n",
    "def my_predict(inputs:torch.Tensor,model:Qwen2ForCausalLM,max_len:int=1024,max_new_tokens:int=512):\n",
    "    for i in range(max_new_tokens):\n",
    "        logits=model.forward(input_ids=inputs)[0]\n",
    "        output_token=torch.argmax(logits[0,-1]).view(-1,1)\n",
    "        inputs=torch.concat([inputs,output_token],dim=1)\n",
    "        if output_token.item()==tokenizer.eos_token_id or inputs.shape[1]>=max_len:\n",
    "            break\n",
    "    return inputs\n",
    "input_ids=model_inputs['input_ids']\n",
    "#print(input_ids)\n",
    "print(f'输入的sentence：\\n{tokenizer.batch_decode(input_ids)[0]}')\n",
    "output_ids=my_predict(inputs=input_ids,model=model)[0][len(input_ids[0]):]\n",
    "output_sentence=tokenizer.decode(output_ids)\n",
    "print(f'输出的sentence：\\n{output_sentence}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
