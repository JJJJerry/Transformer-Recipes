{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 设置hf-mirror镜像站，用于下载Qwen/Qwen2.5-0.5B-Instruct模型\n",
    "import os\n",
    "os.environ['HF_ENDPOINT']='https://hf-mirror.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM,AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "device='cuda:0'\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
       "  \"architectures\": [\n",
       "    \"Qwen2ForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"eos_token_id\": 151645,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 896,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4864,\n",
       "  \"max_position_embeddings\": 32768,\n",
       "  \"max_window_layers\": 21,\n",
       "  \"model_type\": \"qwen2\",\n",
       "  \"num_attention_heads\": 14,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"num_key_value_heads\": 2,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.46.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_sliding_window\": false,\n",
       "  \"vocab_size\": 151936\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    151645,\n",
       "    151643\n",
       "  ],\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.7,\n",
       "  \"top_k\": 20,\n",
       "  \"top_p\": 0.8\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! A large language model (LLM) is an artificial intelligence system that can generate human-like text based on the input it receives. These models are typically trained using large amounts of textual data and can be used for a wide range of tasks such as natural language processing, machine translation, summarization, and more.\n",
      "\n",
      "The development of LLMs has been driven by the increasing availability of text-based knowledge, the growing complexity of natural language, and the need for more efficient ways to process and generate information. With advancements in computing power and computational resources, LLMs have become increasingly powerful and capable of performing complex tasks with high accuracy and speed.\n"
     ]
    }
   ],
   "source": [
    "# Qwen官方例子\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # 加上<|im_start|>assistant\\n\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "# 这里的内部实现应该是一个for循环，一个token一个token地生成\n",
    "with torch.inference_mode():\n",
    "    model_output_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, model_output_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nGive me a short introduction to large language model.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过tokenizer的chat模板把messages更改为对话形式。\n",
    "# 让LLM在这个文本上做续写，就是chat版本模型干的事\n",
    "text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
       "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
       "             13, 151645,    198, 151644,    872,    198,  35127,    752,    264,\n",
       "           2805,  16800,    311,   3460,   4128,   1614,     13, 151645,    198,\n",
       "         151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过tokenizer把刚刚的text分词并转化为数字id\n",
    "# 得到input_ids和attention_mask\n",
    "# 这里的attention_mask全是1\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198,  35127,    752,    264,\n",
      "           2805,  16800,    311,   3460,   4128,   1614,     13, 151645,    198,\n",
      "         151644,  77091,    198,     32,   3460,   4128,   1614,    374,    264,\n",
      "            943,    315,  20443,  11229,    429,    646,   8193,   3738,  12681,\n",
      "           1467,   3118,    389,  11127,   3897,    311,    432,     13,   4220,\n",
      "           4119,    525,   6188,    311,  55359,    279,   5810,   4128,   8692,\n",
      "          16928,    315,  12677,    323,    646,   6923,  55787,    323,   2266,\n",
      "           1832,   8311,  14507,    304,   5257,  30476,   1741,    438,   1467,\n",
      "           9471,     11,  28285,   2022,     11,   3405,  35764,     11,    323,\n",
      "            803,     13,  20286,   4128,   4119,    614,   1012,  13570,   1483,\n",
      "            304,   5043,   1075,   5662,  14468,     11,  25975,   6358,     11,\n",
      "          27682,   5942,     11,    323,    803,     13, 151645]],\n",
      "       device='cuda:0')\n",
      "151645\n"
     ]
    }
   ],
   "source": [
    "# decoder模型输出的ids\n",
    "# 可以发现前面的ids是一样的\n",
    "print(model_output_ids)\n",
    "print(tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Large language models (LLMs) are artificial intelligence systems that can generate human-like text based on a pre-defined set of rules or patterns. These models are designed to mimic the complexity and creativity of human language, allowing them to understand context, learn from previous interactions, and produce coherent responses. LLMs have become increasingly popular in fields such as natural language processing, machine translation, chatbots, and virtual assistants due to their ability to process vast amounts of data quickly and generate meaningful outputs. They have also been used for tasks like image captioning, sentiment analysis, and question answering, demonstrating their potential to automate complex tasks and improve efficiency across various industries.<|im_end|>\n",
      "\n",
      "eos_token:  <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 把模型的输出decode出来看看\n",
    "print(tokenizer.decode(model_output_ids[0]))\n",
    "print(f'\\neos_token:  {tokenizer.eos_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 39])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型forward出来的shape是(batch_size,seq_len,vocab_size)\n",
    "with torch.inference_mode():\n",
    "    model.forward(**model_inputs)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入的sentence：\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "输出的sentence：\n",
      "Certainly! A large language model, or LLM, is a type of artificial intelligence that can generate human-like text based on a given input. These models are designed to be highly accurate, creative, and adaptable, making them useful in a wide range of applications, including but not limited to:\n",
      "\n",
      "1. **Chatbots**: Large language models are often used in chatbots to provide customer service, answer questions, and engage in conversations with users.\n",
      "\n",
      "2. **Text Generation**: They can generate text that is coherent, creative, and even original, which is useful for tasks such as writing articles, creating stories, or even writing books.\n",
      "\n",
      "3. **Sentiment Analysis**: Large language models can analyze and interpret the sentiment of text, which is useful in areas like social media monitoring, customer feedback analysis, and market research.\n",
      "\n",
      "4. **NLP (Natural Language Processing)**: They can process and understand natural language, which is crucial for tasks like language translation, summarization, and text summarization.\n",
      "\n",
      "5. **AI Chatbots**: Large language models are often used in AI chatbots to provide more natural and conversational interactions with users.\n",
      "\n",
      "6. **Educational Tools**: They can be used in educational settings to generate text that is engaging and interactive, which can help students learn and retain information more effectively.\n",
      "\n",
      "7. **Artificial Intelligence**: Large language models are used in AI applications, such as image recognition, speech recognition, and natural language processing.\n",
      "\n",
      "8. **Medical Research**: They can analyze and interpret medical data, which is useful in areas like drug discovery, disease diagnosis, and personalized medicine.\n",
      "\n",
      "9. **Legal Research**: They can generate legal documents and other legal materials based on the input provided.\n",
      "\n",
      "10. **Customer Service**: Large language models can be used in customer service to provide personalized and helpful responses to customers.\n",
      "\n",
      "Large language models are designed to be highly accurate, creative, and adaptable, making them a powerful tool for various applications. They are also subject to ethical considerations, such as the potential for bias and the impact on employment and privacy.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# 一个简单的预测函数，一个token一个token地生成，直到达到最大长度或eos_token。\n",
    "# 这里没有用到temperature、topk、top-p等解码参数\n",
    "@torch.inference_mode()\n",
    "def my_predict(inputs:torch.Tensor,model:Qwen2ForCausalLM,max_len:int=1024,max_new_tokens:int=512):\n",
    "    for i in range(max_new_tokens):\n",
    "        logits=model.forward(input_ids=inputs)[0]\n",
    "        output_token=torch.argmax(logits[0,-1]).view(-1,1)\n",
    "        inputs=torch.concat([inputs,output_token],dim=1)\n",
    "        if output_token.item()==tokenizer.eos_token_id or inputs.shape[1]>=max_len:\n",
    "            break\n",
    "    return inputs\n",
    "input_ids=model_inputs['input_ids']\n",
    "#print(input_ids)\n",
    "print(f'输入的sentence：\\n{tokenizer.batch_decode(input_ids)[0]}')\n",
    "output_ids=my_predict(inputs=input_ids,model=model)[0][len(input_ids[0]):]\n",
    "output_sentence=tokenizer.decode(output_ids)\n",
    "print(f'输出的sentence：\\n{output_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 试一下base模型\n",
    "from transformers import Qwen2ForCausalLM,AutoTokenizer\n",
    "import torch\n",
    "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "device='cuda:0'\n",
    "model = Qwen2ForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) are artificial intelligence systems that can generate human-like text. They are designed to understand and generate text based on a large amount of data, and they are often used for tasks such as text generation, summarization, and translation. LLMs are particularly useful for tasks that require high-level reasoning and creativity, such as writing, writing reviews, and writing articles. They are also used in natural language processing (NLP) tasks, such as sentiment analysis and question answering. Overall, LLMs are a powerful tool for automating tasks that require human-like reasoning and creativity.\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      " Comey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 若用base模型做chat任务\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # 加上<|im_start|>assistant\\n\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "# 这里的内部实现应该是一个for循环，一个token一个token地生成\n",
    "with torch.inference_mode():\n",
    "    model_output_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, model_output_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 132])\n",
      "窗前明月光，____。\n",
      "A. 不是花中偏爱菊\n",
      "B. 举头望明月\n",
      "C. 举杯邀明月\n",
      "D. 举杯邀明月\n",
      "答案:\n",
      "B\n",
      "\n",
      "下列关于“三会一课”制度说法正确的是____。\n",
      "A. 党支部应当组织党员按期参加党员大会、党小组会和上党课，定期召开党支部委员会会议\n",
      "B. 党支部应当组织党员按期参加民主评议\n",
      "C. 党支部应当指定1名以上有表决权的党员作评议对象\n",
      "D. 党支部应当每月开展\n"
     ]
    }
   ],
   "source": [
    "#直接续写\n",
    "inputs=tokenizer([\"窗前明月\"],return_tensors='pt').to(model.device)\n",
    "model_output_ids=model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "# 但好像每次都会生成到指定的最大长度，没有生成到eos_token\n",
    "print(model_output_ids.shape)\n",
    "output_text=tokenizer.batch_decode(model_output_ids)[0]\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
