{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq\n",
    "使用pytorch官方的LSTM和自制的数据集，实现Seq2seq任务的训练和预测。  \n",
    "在这个demo里，输入长度和输出长度不一定一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立Dataloader\n",
    "\n",
    "from toy_datasets import get_dataset_AddSeq\n",
    "\n",
    "# 这里只考虑batch_first=True的情况\n",
    "data_num = 5000\n",
    "dataset, collate_fn,(index_bos,index_eos,index_pad,index_add,vocab_size) = get_dataset_AddSeq(\n",
    "    data_num=data_num\n",
    ") # 一个带符号的个位数加法任务的数据集\n",
    "\n",
    "train_size = 0.2\n",
    "train_dataset = dataset[: int(data_num * train_size)]\n",
    "eval_dataset = dataset[int(data_num * train_size) :]\n",
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=eval_dataset, batch_size=batch_size, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[12,  9, 10,  2, 13],\n",
       "         [12,  0, 10,  4, 13],\n",
       "         [12,  2, 10,  3, 13],\n",
       "         [12,  1, 10,  0, 13],\n",
       "         [12,  9, 10,  6, 13],\n",
       "         [12,  6, 10,  3, 13],\n",
       "         [12,  0, 10,  1, 13],\n",
       "         [12,  1, 10,  2, 13],\n",
       "         [12,  6, 10,  2, 13],\n",
       "         [12,  2, 10,  7, 13],\n",
       "         [12,  9, 10,  1, 13],\n",
       "         [12,  1, 10,  0, 13],\n",
       "         [12,  0, 10,  6, 13],\n",
       "         [12,  0, 10,  5, 13],\n",
       "         [12,  8, 10,  7, 13],\n",
       "         [12,  9, 10,  9, 13]]),\n",
       " 'tgt': tensor([[12,  1,  1, 13],\n",
       "         [12,  4, 13, 11],\n",
       "         [12,  5, 13, 11],\n",
       "         [12,  1, 13, 11],\n",
       "         [12,  1,  5, 13],\n",
       "         [12,  9, 13, 11],\n",
       "         [12,  1, 13, 11],\n",
       "         [12,  3, 13, 11],\n",
       "         [12,  8, 13, 11],\n",
       "         [12,  9, 13, 11],\n",
       "         [12,  1,  0, 13],\n",
       "         [12,  1, 13, 11],\n",
       "         [12,  6, 13, 11],\n",
       "         [12,  5, 13, 11],\n",
       "         [12,  1,  5, 13],\n",
       "         [12,  1,  8, 13]])}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataloader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hidden_dim,num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(num_embeddings=vocab_size,embedding_dim=embed_dim)\n",
    "        self.lstm=nn.LSTM(input_size=embed_dim,hidden_size=hidden_dim,num_layers=num_layers,batch_first=True)\n",
    "    def forward(self,src):\n",
    "        x=self.embedding(src)\n",
    "        lstm_output,(hidden_states,cell)=self.lstm(x)\n",
    "        return hidden_states,cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hidden_dim,num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm=nn.LSTM(input_size=embed_dim,hidden_size=hidden_dim,num_layers=num_layers,batch_first=True)\n",
    "        self.embedding=nn.Embedding(num_embeddings=vocab_size,embedding_dim=embed_dim)\n",
    "        self.head=nn.Linear(hidden_dim,vocab_size) # 分类头\n",
    "    def forward(self,tgt,hidden_states,cell):\n",
    "      \n",
    "        x=self.embedding(tgt)\n",
    "        lstm_output,(hidden_states,cell)=self.lstm(x,(hidden_states,cell)) \n",
    "        output=self.head(lstm_output)\n",
    "        return output,(hidden_states,cell) \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim,hidden_dim,num_layers,teacher_ratio):\n",
    "        super().__init__()\n",
    "        self.teacher_ratio=teacher_ratio # teacher forcing的概率\n",
    "        self.encoder=Encoder(vocab_size=vocab_size,embed_dim=embed_dim,hidden_dim=hidden_dim,num_layers=num_layers)\n",
    "        self.decoder=Decoder(vocab_size=vocab_size,embed_dim=embed_dim,hidden_dim=hidden_dim,num_layers=num_layers)\n",
    "    def forward(self,src,tgt):\n",
    "        batch_size,tgt_seq_len=tgt.shape\n",
    "        hidden_states,cell=self.encoder(src) # encoder先对输入进行编码，得到hidden_states和cell\n",
    "        last_output_ids=tgt[:,0].unsqueeze(1) # batch中，每个序列的第一个token作为起始输入\n",
    "        # (batch_size,1)\n",
    "        output=[]\n",
    "        for i in range(1,tgt_seq_len):\n",
    "            # 输入保存着的hidden_states、cell和这次的输入，预测下一个token的logits\n",
    "            logits,(hidden_states,cell)=self.decoder(last_output_ids,hidden_states,cell) \n",
    "            # logits: (batch_size,1,vocab_size)\n",
    "            output.append(logits.squeeze(1))\n",
    "            if random.random() < self.teacher_ratio:\n",
    "                last_output_ids=tgt[:,i].unsqueeze(1) # teacher forcing，取真实的target\n",
    "            else :\n",
    "                last_output_ids=torch.argmax(logits,dim=-1) # 取预测的target\n",
    "        return torch.stack(output,dim=1)\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict(self,src,tgt,max_seq_len=10,index_eos=13):\n",
    "        batch_size,tgt_seq_len=tgt.shape\n",
    "        hidden_states,cell=self.encoder(src)\n",
    "        last_output_ids=tgt[:,0].unsqueeze(1) # (batch_size,1)\n",
    "        output=[]\n",
    "        for _ in range(tgt_seq_len,max_seq_len):\n",
    "            logits,(hidden_states,cell)=self.decoder(last_output_ids,hidden_states,cell)\n",
    "            last_output_ids=torch.argmax(logits,dim=-1)\n",
    "            output.append(last_output_ids.item())\n",
    "            if last_output_ids.item()==index_eos: # 预测到了eos，就停止预测\n",
    "                break\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "    \n",
    "hidden_dim=128\n",
    "embed_dim=64\n",
    "num_layers=3\n",
    "device='cuda:0'\n",
    "teacher_ratio=0.5\n",
    "model=Seq2Seq(vocab_size=vocab_size,embed_dim=embed_dim,hidden_dim=hidden_dim,num_layers=num_layers,teacher_ratio=teacher_ratio).to(device)\n",
    "loss_func=nn.CrossEntropyLoss(ignore_index=index_pad) # 在训练时，忽略pad的loss\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, loss_func, optimizer, device):\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "        src=batch['src'].to(device)\n",
    "        tgt=batch['tgt'].to(device)\n",
    "        tgt_output=model(src,tgt)\n",
    "        tgt=tgt[:,1:] # 由于预测的是下一个token，所以需要去掉第一个token\n",
    "        tgt_output=tgt_output.view(-1,tgt_output.shape[-1])\n",
    "        tgt=tgt.reshape(-1)\n",
    "        loss=loss_func(tgt_output,tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / (len(losses) * batch_size)\n",
    "\n",
    "\n",
    "@torch.inference_mode() # 验证的时候关闭梯度计算\n",
    "def eval(model, eval_dataloader, loss_func, device):\n",
    "    losses = []\n",
    "    for batch in eval_dataloader:\n",
    "        src=batch['src'].to(device)\n",
    "        tgt=batch['tgt'].to(device)\n",
    "        tgt_output=model(src,tgt)\n",
    "        tgt=tgt[:,1:]\n",
    "        tgt_output=tgt_output.view(-1,tgt_output.shape[-1])\n",
    "        tgt=tgt.reshape(-1)\n",
    "        loss=loss_func(tgt_output,tgt)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / (len(losses) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.10153274072541131, eval_loss:0.07112701869010925\n",
      "epoch:1, train_loss:0.06079302958789326, eval_loss:0.05005219842493534\n",
      "epoch:2, train_loss:0.04143774811001051, eval_loss:0.03180935980379582\n",
      "epoch:3, train_loss:0.03020581407915978, eval_loss:0.025679446585476398\n",
      "epoch:4, train_loss:0.01945081050138152, eval_loss:0.0145473104827106\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    "    eval_loss = eval(\n",
    "        model=model, eval_dataloader=eval_dataloader, loss_func=loss_func, device=device\n",
    "    )\n",
    "    print(f\"epoch:{epoch}, train_loss:{train_loss}, eval_loss:{eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入序列 tensor([[12,  9, 10,  5, 13]], device='cuda:0')\n",
      "输出序列 [12, 1, 4, 13]\n",
      "9 + 5=[1, 4]\n"
     ]
    }
   ],
   "source": [
    "# 个位数加法\n",
    "a=9\n",
    "b=5\n",
    "def add(a,b):\n",
    "    input_list=[index_bos]+[a,index_add,b]+[index_eos]\n",
    "    tgt_list=[index_bos]\n",
    "    src=torch.tensor(input_list).to(device).unsqueeze(0)\n",
    "    tgt=torch.tensor(tgt_list).to(device).unsqueeze(0)\n",
    "    output=model.predict(src,tgt,max_seq_len=10)\n",
    "    print(f'输入序列 {src}')\n",
    "    print(f'输出序列 {tgt_list+output}')\n",
    "    print(f'{a} + {b}={output[:-1]}')\n",
    "add(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
