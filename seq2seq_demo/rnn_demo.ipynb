{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq-RNN\n",
    "使用pytorch官方的RNN和自制的数据集，实现Seq2seq任务的训练和预测。  \n",
    "这里是一个很简单的demo，输入长度和输出长度是一样的。  \n",
    "实现的功能：重复输入序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_datasets import get_dataset_repeat\n",
    "\n",
    "vocab_size = 24\n",
    "seq_len = 8\n",
    "data_num = 3000\n",
    "batch_first = True\n",
    "\n",
    "dataset, collate_fn = get_dataset_repeat(\n",
    "    data_num=data_num, vocab_size=vocab_size, seq_len=seq_len, batch_first=batch_first\n",
    ") # 一个加法任务的数据集\n",
    "train_size = 0.2\n",
    "train_dataset = dataset[: int(data_num * train_size)]\n",
    "eval_dataset = dataset[int(data_num * train_size) :]\n",
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=eval_dataset, batch_size=batch_size, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[17, 15,  2, 17,  7, 13, 21,  1],\n",
       "         [22,  6,  4, 22,  2, 16, 16, 16],\n",
       "         [10,  4,  5,  9, 19, 21, 16, 15],\n",
       "         [ 6, 18, 14, 10, 18,  6,  1, 11],\n",
       "         [ 1, 10, 18,  3, 12, 19, 17,  0],\n",
       "         [ 5, 10,  6, 13, 22,  1,  7, 22],\n",
       "         [ 0, 15,  7,  1, 17,  8, 14, 22],\n",
       "         [17, 14, 18,  6, 17,  8,  3,  5],\n",
       "         [ 0, 15,  6, 13, 20,  2, 14, 13],\n",
       "         [13, 15, 17, 19,  7, 15, 20, 14],\n",
       "         [10, 22, 16,  4,  9, 16, 14,  3],\n",
       "         [10,  7,  9,  2,  5,  9,  6, 14],\n",
       "         [18, 18, 10, 10, 15, 16, 15,  3],\n",
       "         [ 5,  0, 15,  3, 10, 15, 21,  4],\n",
       "         [12, 20,  2,  0,  6,  3,  8,  2],\n",
       "         [ 2,  2, 17,  6,  2,  7,  6,  8]]),\n",
       " 'tgt': tensor([[17, 15,  2, 17,  7, 13, 21,  1],\n",
       "         [22,  6,  4, 22,  2, 16, 16, 16],\n",
       "         [10,  4,  5,  9, 19, 21, 16, 15],\n",
       "         [ 6, 18, 14, 10, 18,  6,  1, 11],\n",
       "         [ 1, 10, 18,  3, 12, 19, 17,  0],\n",
       "         [ 5, 10,  6, 13, 22,  1,  7, 22],\n",
       "         [ 0, 15,  7,  1, 17,  8, 14, 22],\n",
       "         [17, 14, 18,  6, 17,  8,  3,  5],\n",
       "         [ 0, 15,  6, 13, 20,  2, 14, 13],\n",
       "         [13, 15, 17, 19,  7, 15, 20, 14],\n",
       "         [10, 22, 16,  4,  9, 16, 14,  3],\n",
       "         [10,  7,  9,  2,  5,  9,  6, 14],\n",
       "         [18, 18, 10, 10, 15, 16, 15,  3],\n",
       "         [ 5,  0, 15,  3, 10, 15, 21,  4],\n",
       "         [12, 20,  2,  0,  6,  3,  8,  2],\n",
       "         [ 2,  2, 17,  6,  2,  7,  6,  8]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataloader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_dim=128,rnn_hidden_dim=128,rnn_num_layers=3):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(num_embeddings=vocab_size,embedding_dim=embed_dim)\n",
    "        self.rnn=nn.RNN(input_size=embed_dim,hidden_size=rnn_hidden_dim,num_layers=rnn_num_layers)\n",
    "        self.head=nn.Linear(rnn_hidden_dim,vocab_size)\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x) # (seq_len,batch_size,hidden_dim)\n",
    "        rnn_output,hidden_states=self.rnn(x) # (seq_len,batch_size,hidden_dim)\n",
    "        output=self.head(rnn_output)\n",
    "        return output # (seq_len,batch_size,max_ids)\n",
    "    \n",
    "rnn_hidden_dim=128\n",
    "rnn_num_layers=3\n",
    "embed_dim = 64\n",
    "device='cuda:0'\n",
    "model=MyRNN(vocab_size=vocab_size,embed_dim=embed_dim,rnn_hidden_dim=rnn_hidden_dim,rnn_num_layers=rnn_num_layers).to(device)\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, loss_func, optimizer, device):\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "        src = batch[\"src\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "        logits = model(src)  # output是一个序列，每个元素是一个长度为vocab_size的logits向量\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        tgt = tgt.view(-1)\n",
    "        loss = loss_func(logits, tgt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / (len(losses) * batch_size)\n",
    "\n",
    "\n",
    "@torch.inference_mode() # 验证的时候关闭梯度计算\n",
    "def eval(model, eval_dataloader, loss_func, device):\n",
    "    losses = []\n",
    "    for batch in eval_dataloader:\n",
    "        src = batch[\"src\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "        logits = model(src)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        tgt = tgt.view(-1)\n",
    "        loss = loss_func(logits, tgt)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / (len(losses) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.11418168428108881, eval_loss:0.0277207300812006\n",
      "epoch:1, train_loss:0.008777372134653362, eval_loss:0.0024535128567367793\n",
      "epoch:2, train_loss:0.0016124067570720065, eval_loss:0.001135591024843355\n",
      "epoch:3, train_loss:0.0008881887203499087, eval_loss:0.0007181299885269254\n",
      "epoch:4, train_loss:0.0005925360118884495, eval_loss:0.0005046538570119689\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    "    eval_loss = eval(\n",
    "        model=model, eval_dataloader=eval_dataloader, loss_func=loss_func, device=device\n",
    "    )\n",
    "    print(f\"epoch:{epoch}, train_loss:{train_loss}, eval_loss:{eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原序列 [4, 6, 21, 18, 0]\n",
      "生成的序列 [4, 6, 21, 18, 0]\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def predict(sentence, device):\n",
    "    x_tensor = torch.tensor(sentence).unsqueeze(0).to(device)\n",
    "    logits = model(x_tensor)\n",
    "    # print(logits.shape) # (1,seq_len,vocab_size)\n",
    "    output_id = torch.argmax(logits, dim=-1).squeeze(0)  # 在logits那一维取argmax\n",
    "    print(f\"原序列 {sentence}\")\n",
    "    print(f\"生成的序列 {output_id.tolist()}\")\n",
    "\n",
    "\n",
    "test_sentence = [4, 6, 21, 18, 0]\n",
    "predict(test_sentence, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
