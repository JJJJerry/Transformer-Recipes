{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立Dataloader\n",
    "max_ids=24\n",
    "seq_len=8\n",
    "embed_dim=64\n",
    "def make_dataset(data_num,seq_len):\n",
    "    dataset=[]\n",
    "    for _ in range(data_num):\n",
    "        x_ids=torch.tensor([random.randint(0,15) for i in range(seq_len)])\n",
    "        y_ids=x_ids+1\n",
    "        dataset.append(\n",
    "            {\n",
    "                'x_ids':x_ids,\n",
    "                'y_ids':y_ids\n",
    "            }\n",
    "        )\n",
    "    def collate_fn(batch):\n",
    "        x_ids=[b['x_ids'] for b in batch]\n",
    "        y_ids=[b['y_ids'] for b in batch]\n",
    "        x_ids=nn.utils.rnn.pad_sequence(x_ids)\n",
    "        y_ids=nn.utils.rnn.pad_sequence(y_ids) # (seq_len,batch_size)\n",
    "        # 把seq_len放在第一维的原因是让pytorch可以对每个timestep的结果进行并行计算而不是对每个完整序列的结果进行并行计算。\n",
    "        # 因为seq2seq的任务需要对齐的是每个timestep的结果。别的任务一般都是对齐每条数据的结果。\n",
    "        batch={\n",
    "                'x_ids':x_ids,\n",
    "                'y_ids':y_ids\n",
    "            }\n",
    "        return batch\n",
    "    return dataset,collate_fn\n",
    "train_dataset,collate_fn=make_dataset(data_num=1000,seq_len=seq_len) # 构造一个y=x+1的序列\n",
    "batch_size=16\n",
    "train_dataloader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_ids': tensor([[ 6,  4, 15, 12,  1,  6,  7,  3, 12, 14,  3, 12, 10, 13,  1,  2],\n",
       "         [ 8, 10,  2,  4,  4, 15,  0,  5,  6,  6, 15,  7, 11, 13, 15, 14],\n",
       "         [ 3,  3,  9,  7, 15, 14,  7,  4, 14,  5,  1,  4, 11,  3,  0,  9],\n",
       "         [15, 15,  5, 10, 15, 11,  1,  1,  2, 12, 13,  6,  7, 12,  5,  3],\n",
       "         [ 9, 11, 15,  0,  8,  5,  1, 14,  5,  9,  6, 12, 12,  1, 11,  6],\n",
       "         [15,  2,  2,  0,  8,  7,  2,  0, 12,  1,  6, 14,  6,  0, 13, 12],\n",
       "         [12,  3, 12,  0,  8, 13,  6, 13,  8,  5,  3,  1,  4,  1, 10,  7],\n",
       "         [11, 10,  2, 10,  1, 15,  5, 11, 15, 12, 12,  6,  5, 10, 13, 10]]),\n",
       " 'y_ids': tensor([[ 7,  5, 16, 13,  2,  7,  8,  4, 13, 15,  4, 13, 11, 14,  2,  3],\n",
       "         [ 9, 11,  3,  5,  5, 16,  1,  6,  7,  7, 16,  8, 12, 14, 16, 15],\n",
       "         [ 4,  4, 10,  8, 16, 15,  8,  5, 15,  6,  2,  5, 12,  4,  1, 10],\n",
       "         [16, 16,  6, 11, 16, 12,  2,  2,  3, 13, 14,  7,  8, 13,  6,  4],\n",
       "         [10, 12, 16,  1,  9,  6,  2, 15,  6, 10,  7, 13, 13,  2, 12,  7],\n",
       "         [16,  3,  3,  1,  9,  8,  3,  1, 13,  2,  7, 15,  7,  1, 14, 13],\n",
       "         [13,  4, 13,  1,  9, 14,  7, 14,  9,  6,  4,  2,  5,  2, 11,  8],\n",
       "         [12, 11,  3, 11,  2, 16,  6, 12, 16, 13, 13,  7,  6, 11, 14, 11]])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataloader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 63/63 [00:00<00:00, 142.28it/s, loss=0.00225]\n",
      "Epoch 2/3: 100%|██████████| 63/63 [00:00<00:00, 157.54it/s, loss=0.000645]\n",
      "Epoch 3/3: 100%|██████████| 63/63 [00:00<00:00, 198.93it/s, loss=0.000355]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self,max_ids,embed_dim,rnn_hidden_dim,rnn_num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(num_embeddings=max_ids,embedding_dim=embed_dim)\n",
    "        self.rnn=nn.RNN(input_size=embed_dim,hidden_size=rnn_hidden_dim,num_layers=rnn_num_layers)\n",
    "        self.head=nn.Linear(rnn_hidden_dim,max_ids)\n",
    "    def forward(self,x):\n",
    "        x=self.embedding(x) # (seq_len,batch_size,hidden_dim)\n",
    "        rnn_output,hidden_states=self.rnn(x) # (seq_len,batch_size,hidden_dim)\n",
    "        output=self.head(rnn_output)\n",
    "        return output # (seq_len,batch_size,max_ids)\n",
    "    \n",
    "rnn_hidden_dim=128\n",
    "rnn_num_layers=3\n",
    "device='cuda:0'\n",
    "model=MyRNN(max_ids=max_ids,embed_dim=embed_dim,rnn_hidden_dim=rnn_hidden_dim,rnn_num_layers=rnn_num_layers).to(device)\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "epoch=3\n",
    "\n",
    "for i in range(epoch):\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Epoch {i+1}/{epoch}\") as pbar:\n",
    "        for batch in train_dataloader:\n",
    "            x_ids=batch['x_ids'].to(device)\n",
    "            y_ids=batch['y_ids'].to(device)  \n",
    "            y_output=model(x_ids)\n",
    "            y_ids=y_ids.unsqueeze(2)\n",
    "            y_output=y_output.view(-1,y_output.shape[-1])\n",
    "            y_ids=y_ids.view(-1)\n",
    "            loss=loss_func(y_output,y_ids)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"loss\": loss.item()/batch_size})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 24])\n",
      "tensor([-0.8360, -0.0113,  0.6309,  0.8853, -3.6343,  8.5278,  0.8091,  0.9241,\n",
      "        -0.5562, -2.6830,  0.3736, -1.2310,  1.3955, -0.9789,  0.5743,  0.9856,\n",
      "        -1.6160, -0.9963, -0.3596, -0.9936, -0.7572, -0.8647, -1.0270, -0.6180],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9, 5, 1], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=[4,6,8,4,0]\n",
    "x_tensor=torch.tensor(x_test).to(device)\n",
    "logits=model(x_tensor) # (seq_len,max_ids)\n",
    "print(logits.shape)\n",
    "print(logits[0])\n",
    "output_ids=torch.argmax(logits,dim=1) #在max_ids那一维取argmax\n",
    "output_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
