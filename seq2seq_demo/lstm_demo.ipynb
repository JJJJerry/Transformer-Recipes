{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq-LSTM\n",
    "使用pytorch官方的lstm和自制的数据集，实现Seq2seq任务的训练和预测。  \n",
    "这里是一个很简单的demo，输入长度和输出长度是一样的。  \n",
    "实现的功能：将输入序列的每个元素都加1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_datasets import get_dataset_add1\n",
    "\n",
    "vocab_size = 24\n",
    "seq_len = 8\n",
    "data_num = 3000\n",
    "batch_first = True\n",
    "\n",
    "dataset, collate_fn = get_dataset_add1(\n",
    "    data_num=data_num, vocab_size=vocab_size, seq_len=seq_len, batch_first=batch_first\n",
    ") # 一个加法任务的数据集\n",
    "train_size = 0.2\n",
    "train_dataset = dataset[: int(data_num * train_size)]\n",
    "eval_dataset = dataset[int(data_num * train_size) :]\n",
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, collate_fn=collate_fn\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=eval_dataset, batch_size=batch_size, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'src': tensor([16, 18,  6,  0, 18, 11, 14,  6]),\n",
       "  'tgt': tensor([17, 19,  7,  1, 19, 12, 15,  7])},\n",
       " {'src': tensor([ 0,  0, 15,  1,  3,  9,  6,  1]),\n",
       "  'tgt': tensor([ 1,  1, 16,  2,  4, 10,  7,  2])},\n",
       " {'src': tensor([17, 12, 20, 21, 10,  0,  4,  5]),\n",
       "  'tgt': tensor([18, 13, 21, 22, 11,  1,  5,  6])}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:3]  # 看一下dataset，可以发现tgt是src+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[16, 18,  6,  0, 18, 11, 14,  6],\n",
       "         [ 0,  0, 15,  1,  3,  9,  6,  1],\n",
       "         [17, 12, 20, 21, 10,  0,  4,  5],\n",
       "         [17, 10,  9,  6, 16, 16, 14,  5],\n",
       "         [18,  2,  2,  2,  0, 18,  9, 19],\n",
       "         [ 1, 22, 18, 15,  0,  9, 14, 19],\n",
       "         [16,  6,  7, 22,  3, 11, 21,  9],\n",
       "         [16,  8, 17, 19, 12, 16,  6, 16],\n",
       "         [ 7,  1, 21,  5,  2,  6, 22, 21],\n",
       "         [ 1, 14, 18,  9,  6,  6, 11, 17],\n",
       "         [17, 14,  1, 21,  2, 18,  5,  2],\n",
       "         [11, 11,  0,  0,  8, 13,  0, 11],\n",
       "         [12, 17, 15,  7, 21,  5,  1,  9],\n",
       "         [15, 22, 15,  9,  3, 19, 12,  6],\n",
       "         [ 8, 10, 20, 11, 16,  5,  0, 15],\n",
       "         [ 7, 10, 18, 15,  6, 20, 15,  5]]),\n",
       " 'tgt': tensor([[17, 19,  7,  1, 19, 12, 15,  7],\n",
       "         [ 1,  1, 16,  2,  4, 10,  7,  2],\n",
       "         [18, 13, 21, 22, 11,  1,  5,  6],\n",
       "         [18, 11, 10,  7, 17, 17, 15,  6],\n",
       "         [19,  3,  3,  3,  1, 19, 10, 20],\n",
       "         [ 2, 23, 19, 16,  1, 10, 15, 20],\n",
       "         [17,  7,  8, 23,  4, 12, 22, 10],\n",
       "         [17,  9, 18, 20, 13, 17,  7, 17],\n",
       "         [ 8,  2, 22,  6,  3,  7, 23, 22],\n",
       "         [ 2, 15, 19, 10,  7,  7, 12, 18],\n",
       "         [18, 15,  2, 22,  3, 19,  6,  3],\n",
       "         [12, 12,  1,  1,  9, 14,  1, 12],\n",
       "         [13, 18, 16,  8, 22,  6,  2, 10],\n",
       "         [16, 23, 16, 10,  4, 20, 13,  7],\n",
       "         [ 9, 11, 21, 12, 17,  6,  1, 16],\n",
       "         [ 8, 11, 19, 16,  7, 21, 16,  6]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看一下dataloader\n",
    "# dataloader是一个迭代器，不能直接train_dataloader[0]查看\n",
    "next(train_dataloader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim=128,\n",
    "        lstm_hidden_dim=128,\n",
    "        lstm_num_layers=3,\n",
    "        batch_first=False,\n",
    "    ):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=embed_dim\n",
    "        )  # 嵌入\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_num_layers,\n",
    "            batch_first=batch_first,\n",
    "        )  # 使用torch的LSTM\n",
    "        self.head = nn.Linear(lstm_hidden_dim, vocab_size)  # 线性分类头，根据输出的logits分类\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size,seq_len,hidden_dim)\n",
    "        lstm_output, (hidden_states, cell) = self.lstm(x)  # (batch_size,seq_len,hidden_dim)\n",
    "        output = self.head(lstm_output)\n",
    "        return output  # (batch_size,seq_len,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden_dim = 128\n",
    "embed_dim = 64\n",
    "lstm_num_layers = 3\n",
    "device = \"cuda:0\"\n",
    "model = MyLSTM(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    lstm_hidden_dim=lstm_hidden_dim,\n",
    "    lstm_num_layers=lstm_num_layers,\n",
    "    batch_first=batch_first,\n",
    ").to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, loss_func, optimizer, device):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        src = batch[\"src\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "        logits = model(src)  # output是一个序列，每个元素是一个长度为vocab_size的logits向量\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        tgt = tgt.view(-1)\n",
    "        loss = loss_func(logits, tgt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / (len(losses) * batch_size)\n",
    "\n",
    "\n",
    "@torch.inference_mode() # 验证的时候关闭梯度计算\n",
    "def eval(model, eval_dataloader, loss_func, device):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        src = batch[\"src\"].to(device)\n",
    "        tgt = batch[\"tgt\"].to(device)\n",
    "        logits = model(src)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        tgt = tgt.view(-1)\n",
    "        loss = loss_func(logits, tgt)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / (len(losses) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.19413185237269653, eval_loss:0.17754725565512974\n",
      "epoch:1, train_loss:0.0988553432061484, eval_loss:0.028643952620526155\n",
      "epoch:2, train_loss:0.011248174650398525, eval_loss:0.004105844631170233\n",
      "epoch:3, train_loss:0.002652483719621638, eval_loss:0.0018517607264220714\n",
      "epoch:4, train_loss:0.0014275532969469694, eval_loss:0.001156220156699419\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        loss_func=loss_func,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "    )\n",
    "    eval_loss = eval(\n",
    "        model=model, eval_dataloader=eval_dataloader, loss_func=loss_func, device=device\n",
    "    )\n",
    "    print(f\"epoch:{epoch}, train_loss:{train_loss}, eval_loss:{eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原序列 [4, 6, 21, 18, 0]\n",
      "生成的序列 [5, 7, 22, 19, 1]\n"
     ]
    }
   ],
   "source": [
    "@torch.inference_mode()\n",
    "def predict(sentence, device):\n",
    "    x_tensor = torch.tensor(sentence).unsqueeze(0).to(device)\n",
    "    logits = model(x_tensor)\n",
    "    # print(logits.shape) # (1,seq_len,vocab_size)\n",
    "    output_id = torch.argmax(logits, dim=-1).squeeze(0)  # 在logits那一维取argmax\n",
    "    print(f\"原序列 {sentence}\")\n",
    "    print(f\"生成的序列 {output_id.tolist()}\")\n",
    "\n",
    "\n",
    "test_sentence = [4, 6, 21, 18, 0]\n",
    "predict(test_sentence, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
